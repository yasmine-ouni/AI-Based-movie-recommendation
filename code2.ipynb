{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Step 1: Load the data and perform EDA (keeping your existing EDA function)\n",
    "df = pd.read_csv('D:/movie recommendation/titles.csv')\n",
    "df = perform_eda(df)  # Assuming perform_eda is defined elsewhere\n",
    "\n",
    "# Step 2: Initialize LangChain with ChromaDB and Sentence Transformers\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def initialize_langchain_with_chroma(df, batch_size=5461):\n",
    "    \"\"\"\n",
    "    Initializes LangChain with ChromaDB using SentenceTransformer embeddings.\n",
    "    \"\"\"\n",
    "    # Create embedding model\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Initialize Chroma vector store with LangChain\n",
    "    chroma_db = Chroma(embedding_function=embedding_model)\n",
    "    \n",
    "    # Add movie data to ChromaDB in batches\n",
    "    documents = []\n",
    "    for idx, row in df.iterrows():\n",
    "        # Combine features into a single text string for embedding\n",
    "        description = row.get('description', 'No description available')\n",
    "        genres = ', '.join(row['genres']) if isinstance(row['genres'], list) else row.get('genres', 'Unknown genres')\n",
    "        imdb_score = row.get('imdb_score', 'Unknown')\n",
    "        release_year = row.get('release_year', 'Unknown')\n",
    "        \n",
    "        combined_text = f\"{description} | Genres: {genres} | IMDb Score: {imdb_score} | Release Year: {release_year}\"\n",
    "        \n",
    "        # Create a Document object with an id\n",
    "        document = Document(\n",
    "            page_content=combined_text,\n",
    "            metadata={\n",
    "                'title': row['title'],\n",
    "                'genres': genres,\n",
    "                'imdb_score': imdb_score,\n",
    "                'release_year': release_year\n",
    "            },\n",
    "            id=str(idx)  # Use a unique ID, for example, the DataFrame index\n",
    "        )\n",
    "        \n",
    "        documents.append(document)\n",
    "        \n",
    "        # When the batch size reaches the limit, add to the vector store\n",
    "        if len(documents) >= batch_size:\n",
    "            chroma_db.add_documents(documents)\n",
    "            documents = []  # Reset for the next batch\n",
    "    \n",
    "    # Add any remaining documents\n",
    "    if documents:\n",
    "        chroma_db.add_documents(documents)\n",
    "    \n",
    "    return chroma_db\n",
    "\n",
    "# Initialize LangChain with ChromaDB\n",
    "chroma_db = initialize_langchain_with_chroma(df)\n",
    "\n",
    "\n",
    "# Step 3: Set up the LLM and RetrievalQA Chain\n",
    "def initialize_llm_and_retrievalqa():\n",
    "    \"\"\"\n",
    "    Set up the LLM for response generation and the RetrievalQA chain.\n",
    "    \"\"\"\n",
    "    # Initialize Hugging Face model and tokenizer\n",
    "    hf_token = \"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\"  # Replace with your Hugging Face token\n",
    "    model_name = \"google/gemma-2-2b-it\"\n",
    "\n",
    "    # Initialize the Hugging Face LLM with LangChain\n",
    "    llm = HuggingFaceHub(repo_id=model_name, huggingfacehub_api_token=hf_token)\n",
    "\n",
    "    # Use the LangChain RetrievalQA chain to generate answers\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=chroma_db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    return qa_chain\n",
    "\n",
    "# Initialize the LLM and RetrievalQA Chain\n",
    "qa_chain = initialize_llm_and_retrievalqa()\n",
    "\n",
    "# Step 4: Generate Response\n",
    "def generate_response(user_input, qa_chain):\n",
    "    \"\"\"\n",
    "    Generate a response using LangChain's RetrievalQA based on user input.\n",
    "    This function returns only the 'result' from the response, which contains\n",
    "    the generated answer to the user's query.\n",
    "    \"\"\"\n",
    "    # Use invoke() to capture both outputs if necessary\n",
    "    response = qa_chain.invoke({\"query\": user_input})\n",
    "    \n",
    "    # Extract the 'result' from the response dictionary\n",
    "    answer = response.get('result', 'No answer found.')\n",
    "    \n",
    "    return answer\n",
    "\n",
    "\n",
    "# Example user input and response generation\n",
    "user_input = \"I want a horror movie\"\n",
    "response = generate_response(user_input, qa_chain)\n",
    "print(\"Generated response:\\n\", response)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
